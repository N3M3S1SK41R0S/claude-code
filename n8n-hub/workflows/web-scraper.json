{
  "name": "NEMESIS - Web Scraper via External API (FIXED)",
  "nodes": [
    {
      "parameters": {},
      "id": "scraper-webhook",
      "name": "Scrape Request",
      "type": "n8n-nodes-base.webhook",
      "typeVersion": 1,
      "position": [250, 400],
      "webhookId": "web-scraper"
    },
    {
      "parameters": {
        "functionCode": "// Web Scraper Request Handler\n// FIXED: Uses external APIs instead of require('cheerio')\n// n8n Code nodes cannot use npm packages like cheerio, puppeteer\n// Options:\n//   1. ScrapingBee - JavaScript rendering ($49/month)\n//   2. Apify - Full scraping platform (pay-per-use)\n//   3. Browserless - Headless Chrome API\n//   4. Simple HTTP for static HTML (free, limited)\n\nconst input = $input.first().json;\n\nconst request = {\n  request_id: 'SCRAPE-' + Date.now(),\n  timestamp: new Date().toISOString(),\n  url: input.url,\n  method: input.method || 'simple',\n  selectors: input.selectors || {},\n  options: {\n    render_js: input.render_js || false,\n    wait_for: input.wait_for || null,\n    screenshot: input.screenshot || false,\n    proxy: input.use_proxy || false\n  }\n};\n\nif (!request.url) {\n  request.valid = false;\n  request.error = 'URL is required';\n} else {\n  request.valid = true;\n}\n\nreturn [{ json: request }];"
      },
      "id": "prepare",
      "name": "Prepare Request",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [500, 400]
    },
    {
      "parameters": {
        "conditions": {
          "boolean": [{ "value1": "={{ $json.options.render_js }}", "value2": true }]
        }
      },
      "id": "check-js",
      "name": "Needs JS Render?",
      "type": "n8n-nodes-base.if",
      "typeVersion": 1,
      "position": [750, 400]
    },
    {
      "parameters": {
        "url": "={{ $json.url }}",
        "options": {
          "timeout": 30000,
          "redirect": { "redirect": { "followRedirects": true } }
        }
      },
      "id": "simple-fetch",
      "name": "Simple HTTP Fetch",
      "type": "n8n-nodes-base.httpRequest",
      "typeVersion": 4,
      "position": [1000, 500]
    },
    {
      "parameters": {
        "url": "https://app.scrapingbee.com/api/v1/",
        "sendQuery": true,
        "queryParameters": {
          "parameters": [
            { "name": "api_key", "value": "={{ $env.SCRAPINGBEE_API_KEY }}" },
            { "name": "url", "value": "={{ $json.url }}" },
            { "name": "render_js", "value": "true" },
            { "name": "extract_rules", "value": "={{ JSON.stringify($json.selectors) }}" }
          ]
        },
        "options": { "timeout": 60000 }
      },
      "id": "scrapingbee",
      "name": "Scrape via ScrapingBee",
      "type": "n8n-nodes-base.httpRequest",
      "typeVersion": 4,
      "position": [1000, 250]
    },
    {
      "parameters": {
        "functionCode": "// Parse HTML response without cheerio\n// Uses regex and string methods (works in n8n Code node)\nconst html = typeof $input.first().json === 'string' \n  ? $input.first().json \n  : $input.first().json.data || $input.first().json.body || '';\nconst request = $('Prepare Request').first().json;\n\nconst result = {\n  request_id: request.request_id,\n  url: request.url,\n  timestamp: new Date().toISOString(),\n  data: {}\n};\n\n// Simple regex-based extraction (works without cheerio)\nconst extractors = {\n  title: /<title[^>]*>([^<]+)<\\/title>/i,\n  h1: /<h1[^>]*>([^<]+)<\\/h1>/gi,\n  h2: /<h2[^>]*>([^<]+)<\\/h2>/gi,\n  description: /<meta[^>]*name=[\"']description[\"'][^>]*content=[\"']([^\"']+)[\"']/i,\n  ogTitle: /<meta[^>]*property=[\"']og:title[\"'][^>]*content=[\"']([^\"']+)[\"']/i,\n  ogDescription: /<meta[^>]*property=[\"']og:description[\"'][^>]*content=[\"']([^\"']+)[\"']/i,\n  ogImage: /<meta[^>]*property=[\"']og:image[\"'][^>]*content=[\"']([^\"']+)[\"']/i\n};\n\n// Extract title\nconst titleMatch = html.match(extractors.title);\nresult.data.title = titleMatch ? titleMatch[1].trim() : null;\n\n// Extract meta description\nconst descMatch = html.match(extractors.description);\nresult.data.description = descMatch ? descMatch[1] : null;\n\n// Extract headings\nresult.data.h1 = [];\nlet h1Match;\nwhile ((h1Match = extractors.h1.exec(html)) !== null) {\n  result.data.h1.push(h1Match[1].trim());\n}\n\nresult.data.h2 = [];\nlet h2Match;\nwhile ((h2Match = extractors.h2.exec(html)) !== null) {\n  result.data.h2.push(h2Match[1].trim());\n}\n\n// Extract OpenGraph\nresult.data.opengraph = {};\nconst ogTitleMatch = html.match(extractors.ogTitle);\nif (ogTitleMatch) result.data.opengraph.title = ogTitleMatch[1];\nconst ogDescMatch = html.match(extractors.ogDescription);\nif (ogDescMatch) result.data.opengraph.description = ogDescMatch[1];\nconst ogImageMatch = html.match(extractors.ogImage);\nif (ogImageMatch) result.data.opengraph.image = ogImageMatch[1];\n\n// Extract links (first 20)\nresult.data.links = [];\nconst linkRegex = /<a[^>]*href=[\"']([^\"']+)[\"'][^>]*>([^<]*)<\\/a>/gi;\nlet linkMatch;\nlet linkCount = 0;\nwhile ((linkMatch = linkRegex.exec(html)) !== null && linkCount < 20) {\n  if (linkMatch[1] && !linkMatch[1].startsWith('#')) {\n    result.data.links.push({ href: linkMatch[1], text: linkMatch[2].trim() });\n    linkCount++;\n  }\n}\n\n// Stats\nresult.stats = {\n  html_length: html.length,\n  links_found: result.data.links.length,\n  h1_count: result.data.h1.length,\n  h2_count: result.data.h2.length\n};\n\nresult.success = true;\n\nreturn [{ json: result }];"
      },
      "id": "parse-html",
      "name": "Parse HTML (No Cheerio)",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [1250, 400]
    },
    {
      "parameters": {
        "mode": "raw",
        "jsonOutput": "={{ JSON.stringify($json) }}"
      },
      "id": "respond",
      "name": "Return Data",
      "type": "n8n-nodes-base.respondToWebhook",
      "typeVersion": 1,
      "position": [1500, 400]
    }
  ],
  "connections": {
    "Scrape Request": {
      "main": [[{ "node": "Prepare Request", "type": "main", "index": 0 }]]
    },
    "Prepare Request": {
      "main": [[{ "node": "Needs JS Render?", "type": "main", "index": 0 }]]
    },
    "Needs JS Render?": {
      "main": [
        [{ "node": "Scrape via ScrapingBee", "type": "main", "index": 0 }],
        [{ "node": "Simple HTTP Fetch", "type": "main", "index": 0 }]
      ]
    },
    "Simple HTTP Fetch": {
      "main": [[{ "node": "Parse HTML (No Cheerio)", "type": "main", "index": 0 }]]
    },
    "Scrape via ScrapingBee": {
      "main": [[{ "node": "Parse HTML (No Cheerio)", "type": "main", "index": 0 }]]
    },
    "Parse HTML (No Cheerio)": {
      "main": [[{ "node": "Return Data", "type": "main", "index": 0 }]]
    }
  },
  "settings": { "executionOrder": "v1" },
  "tags": ["nemesis", "scraper", "web", "fixed"],
  "notes": "FIXED: Uses regex parsing instead of cheerio (not available in n8n). For JS-rendered sites, uses ScrapingBee API. Set SCRAPINGBEE_API_KEY for JS rendering."
}
